# -*- coding: utf-8 -*-
"""Simple-EDA_copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zDgiDkYYrWyFe8MSQ42nrJp5xoaQClNJ

## Imports
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
from pandas.api.types import is_numeric_dtype

from google.colab import drive
drive.mount('./drive')

"""## Dataset Import"""

raw_h1b_df = pd.read_csv('./drive/Shareddrives/550/datasets/raw_data/h1b_data.csv')

raw_company_reviews_df = pd.read_csv('./drive/Shareddrives/550/datasets/raw_data/company_reviews.csv')

raw_h1b_df.columns

raw_company_reviews_df.columns

"""# EDA

### Drop unused Columns

Identify the columns that are not used for our purpose and can be dropped.
"""

company_drop = ['ceo_approval', 'ceo_count', 'roles', 'salary', 
                'interview_count', 'headquarters', 'revenue', 'website']

h1b_drop = ['emp_zip', 'emp_country', 'soc_code', 'soc_name',
            'prevailing_wage', 'pw_unit', 'pw_level', 'wage_to',
            'wage_unit', 'work_city', 'work_state', 'emp_h1b_dependent',
            'emp_willful_violator', 'lat', 'lng']

raw_h1b_filtered_df = raw_h1b_df.drop(h1b_drop, axis=1)

raw_company_reviews_filtered_df = raw_company_reviews_df.drop(company_drop, axis=1)

"""### EDA helper functions"""

# Analyze the NULL value percentage in dataset
def NullPercentage(df):
  print("Dataset NULL value percentage.")
  print(df.apply(lambda col : col.isnull().sum()/len(col)))
  print("\n")

def CompanyNameCleanup(name):
  ret = name.lower().replace(", inc.", "").replace(",inc.", "").replace(", inc", "").replace(" inc.", "").replace(",inc", "").replace(" inc", "")
  ret = ret.replace(", llp.", "").replace(",llp.", "").replace(", llp", "").replace(" llp.", "").replace(",llp", "").replace(" llp", "")
  ret = ret.replace(", llc.", "").replace(",llc.", "").replace(", llc", "").replace(" llc.", "").replace(",llc", "").replace(" llc", "")
  return ret

"""### H1-B Database EDA"""

NullPercentage(raw_h1b_filtered_df)

"""##### EDA analysis
As all fields in the H1-B database are important for the purpose of our study, we need to drop entries that have at least one fields being NULL
"""

h1b_df = raw_h1b_filtered_df.dropna()

NullPercentage(h1b_df)

h1b_df.dtypes

"""Size of the H1B data after dropping NULL values is:"""

len(h1b_df)

"""##### Process and Cleanup String Values

**full_time_position** field

We will modify the `full_time_position` field to be boolean for easier query later
"""

h1b_df.loc[:, 'full_time_position'] = h1b_df['full_time_position'].apply(lambda x : True if x == 'Y' else False)

"""**emp_name** field"""

h1b_df['emp_name'].unique

h1b_df[h1b_df['emp_name'] == 'zionsville maria montessori international acad..']['emp_name'] = 'zionsville maria montessori international academy'

h1b_df.loc[:, 'emp_name'] = h1b_df['emp_name'].apply(lambda x : CompanyNameCleanup(x))

# h1b_filtered_df['emp_name'] = h1b_filtered_df['emp_name'].apply(lambda x : x.lower())
# h1b_filtered_df['emp_name'] = h1b_filtered_df['emp_name'].apply(lambda x : x.replace(", inc.", " inc"))
# h1b_filtered_df['emp_name'] = h1b_filtered_df['emp_name'].apply(lambda x : x.replace(", llp", " llp"))
# h1b_filtered_df['emp_name'] = h1b_filtered_df['emp_name'].apply(lambda x : x.replace(", llc", " llc"))

h1b_df.info()

"""**emp_city** field"""

h1b_df['emp_city'].unique

h1b_df.loc[:, 'emp_city'] = h1b_df['emp_city'].apply(lambda x : x.lower())

"""**job_title** field"""

h1b_df.loc[:, 'job_title'] = h1b_df['job_title'].apply(lambda x : str(x).lower())

h1b_df.info()

"""### Company Reviews Database EDA"""

NullPercentage(raw_company_reviews_filtered_df)

"""##### EDA analysis
We noticed that there are multiple fields having NULL values. Attributes such as `name`, `rating` are important values for our analysis, and entries with these fields being NULL values should be dropped. On the other hand, fields such as `reviews`(reviews breakdown), `description`(text reviews), interview related stats, `employees`(number of employees), and `industry` are good-to-have values that will improve how informative our analysis would be, and it is ok for them to be NULL for certain companies. 
"""

company_reviews_df = raw_company_reviews_filtered_df.dropna(subset=['name', 'rating'])

"""Size of company review data after dropping NULL value is: """

len(company_reviews_df)

"""##### Process and Cleanup String Values

###### name
"""

company_reviews_df['name'].unique()

company_reviews_df.loc[:, 'name'] = company_reviews_df['name'].apply(lambda x : CompanyNameCleanup(x))

company_reviews_df['name'].unique()

"""##### industry"""

company_reviews_df['industry'].unique()

"Restaurants, Travel and Leisure\nRestaurants".replace('\n', '/')

company_reviews_df.loc[:, 'industry'] = company_reviews_df['industry'].apply(lambda x : str(x).replace('\n', '/'))

company_reviews_df['industry'].unique()

"""Size of dataframe after processing:"""

len(company_reviews_df)

company_reviews_df.info()

"""### Attempt to merge"""

# merged_df = pd.merge(
#     h1b_df, company_reviews_df, left_on="emp_name", right_on='name', how="inner", suffixes=('_left', '_right')
# )

# len(merged_df)

"""## Explode Nested Fields for Main Datasets

Company reviews dataset
"""

company_reviews_exploded_df = company_reviews_df.copy()

company_reviews_df['locations']

company_reviews_df[company_reviews_df['name'] == 'google']

company_reviews_exploded_df.loc[:, 'locations'] = company_reviews_df['locations'].apply(lambda x : re.findall("\'([A-Za-z\\s]+),\\s([A-Z]{2})\':\\s", x))

company_reviews_exploded_df = company_reviews_exploded_df.explode('locations')

company_reviews_exploded_df['locations']

re.findall("\(\'([A-Za-z\\s]+)\',\\s\'[A-Z]{2}\'\)", "('Scottsdale', 'AZ')")[0]

company_reviews_exploded_df['city'] = company_reviews_exploded_df['locations'].apply(lambda x : re.findall("\(\'([A-Za-z\\s]+)\',\\s\'[A-Z]{2}\'\)", str(x)))

company_reviews_exploded_df['city']

company_reviews_exploded_df.loc[:, 'city'] = company_reviews_exploded_df['city'].apply(lambda x : x[0] if len(x) > 0 else np.NaN)

company_reviews_exploded_df['state'] = company_reviews_exploded_df['locations'].apply(lambda x : re.findall("\(\'[A-Za-z\\s]+\',\\s\'([A-Z]{2})\'\)", str(x)))

company_reviews_exploded_df.loc[:, 'state']  = company_reviews_exploded_df['state'].apply(lambda x : x[0] if len(x) > 0 else np.NaN)

company_reviews_exploded_df = company_reviews_exploded_df.drop('locations', axis=1)

"""## Split Into Tables"""

company_reviews_df.columns

"""#### Industry"""

industry_df = pd.DataFrame({"industryId": range(0, len(company_reviews_df['industry'].unique())), 
                            "industry": company_reviews_df['industry'].unique()})

industry_df = industry_df[industry_df['industry'] != 'nan']

industry_df = industry_df.drop_duplicates().reset_index(drop=True)

industry_df.dtypes

industry_df.head(3)

"""#### Location"""

h1b_df.columns

h1b_df.loc[:, 'emp_city'] = h1b_df['emp_city'].apply(lambda x : x.split(",")[0])

location_df = pd.DataFrame({ "city": h1b_df['emp_city'], 
                            "state": h1b_df['emp_state']})

location_df = location_df.reset_index(drop=True)

location_df = pd.concat([location_df, company_reviews_exploded_df[['city', 'state']]])

location_df = location_df.drop_duplicates(subset=['city', 'state']).reset_index(drop=True)

location_df = location_df.drop_duplicates(subset=['city', 'state']).reset_index(drop=True)

location_df.insert(loc=0, column='locationId', value= range(0, len(location_df)))

location_df.dtypes

"""#### Job"""

job_df = pd.DataFrame({ "title": h1b_df['job_title'], 
                       "fulltime": h1b_df['full_time_position']})

job_df = job_df.drop_duplicates(subset=['title', 'fulltime']).reset_index(drop=True)

job_df.insert(loc=0, column='jobId', value= range(0, len(job_df)))

job_df.dtypes

"""#### Company"""

company_df = pd.DataFrame({ "name": company_reviews_exploded_df['name'], 
                            "employeeSize": company_reviews_exploded_df['employees'],
                            "industry": company_reviews_exploded_df['industry']})

temp_df = h1b_df[['emp_name']].copy()

temp_df = temp_df.rename(columns={'emp_name': 'name'}).dropna()

company_df = pd.concat([company_df, temp_df]).drop_duplicates(subset=['name']).reset_index(drop=True)

company_df.insert(loc=0, column='companyId', value= range(0, len(company_df)))

industry_dict = {}
for index, row in industry_df.iterrows():
  industry_dict[row['industry']] = row['industryId']

company_df['industryId'] = company_df['industry'].apply(lambda x : industry_dict[str(x)] if str(x) != 'nan' else np.nan)

company_df['industryId'] = pd.Series(company_df['industryId']).astype('Int64')

company_df['name'] = company_df['name'].astype('string')

company_df['employeeSize'] = company_df['employeeSize'].astype('string')

company_df['industry'] = company_df['industry'].astype('string')

company_df

"""#### Reviews"""

company_dict = {}
for index, row in company_df[['name', 'companyId']].iterrows():
  company_dict[row['name']] = row['companyId']

reviews_df = pd.DataFrame({ "name": company_reviews_exploded_df['name'], 
                            "overallRating": company_reviews_exploded_df['rating'],
                            "textReview": company_reviews_exploded_df['description'],
                            "workLifeBalance": company_reviews_exploded_df['ratings'].apply(lambda x : re.findall("\'Work/Life Balance\':\\s\'([0-9\.]+)'", str(x))),
                            "compensationOrBenefits": company_reviews_exploded_df['ratings'].apply(lambda x : re.findall("\'Compensation/Benefits\':\\s\'([0-9\.]+)'", str(x))),
                            "jobSecurityOrAdvance": company_reviews_exploded_df['ratings'].apply(lambda x : re.findall("\'Job Security/Advancement\':\\s\'([0-9\.]+)'", str(x))),
                            "management": company_reviews_exploded_df['ratings'].apply(lambda x : re.findall("\'Management\':\\s\'([0-9\.]+)'", str(x))),
                            "culture": company_reviews_exploded_df['ratings'].apply(lambda x : re.findall("\'Culture\':\\s\'([0-9\.]+)'", str(x))),
                            "happiness": company_reviews_exploded_df['happiness'].apply(lambda x : re.findall("\'Work Happiness Score\':\\s\'([0-9\.]+)'", str(x))), 
                            "numReviews": company_reviews_exploded_df['reviews'].apply(lambda x : re.findall("([0-9]+)\\sreviews", str(x)))})

ratings_cols = ['workLifeBalance', 'compensationOrBenefits', 'jobSecurityOrAdvance', 'management', 'culture', 'happiness', 'numReviews']
for col in ratings_cols:
  reviews_df.loc[:, col] = reviews_df[col].apply(lambda x : x[0] if len(x) > 0 else np.NaN)

reviews_df['companyId'] = reviews_df['name'].apply(lambda x : company_dict[str(x)] if str(x) != 'nan' else np.nan)

reviews_df['companyId'] = pd.Series(reviews_df['companyId']).astype('Int64')

reviews_df = reviews_df.drop(columns=['name'])

reviews_df.insert(loc=0, column='reviewId', value= range(0, len(reviews_df)))

reviews_df.head(3)

reviews_df.dtypes

"""#### InterviewReview"""

def mapInterviewDurationToTime(duration_str):
  if duration_str == 'About a day or two':
    return 2
  elif duration_str == 'About a week':
    return 7
  elif duration_str == 'About two weeks':
    return 14
  elif duration_str == 'About a month':
    return 10
  elif duration_str == 'More than one month':
    return 60
  else:
    return np.NaN

company_reviews_exploded_df.loc[:, 'interview_duration'] = company_reviews_exploded_df['interview_duration'].apply(lambda x : mapInterviewDurationToTime(x))

interviewReview_df = pd.DataFrame({ "name": company_reviews_exploded_df['name'], 
                                    "timeline(days)": company_reviews_exploded_df['interview_duration'],
                                    "experience": company_reviews_exploded_df['interview_experience'],
                                    "difficulty": company_reviews_exploded_df['interview_difficulty']})

interviewReview_df['companyId'] = interviewReview_df['name'].apply(lambda x : company_dict[str(x)] if str(x) != 'nan' else np.nan)

interviewReview_df['companyId'] = pd.Series(interviewReview_df['companyId']).astype('Int64')

interviewReview_df = interviewReview_df.drop(columns=['name'])

interviewReview_df.insert(loc=0, column='interviewReviewId', value= range(0, len(interviewReview_df)))

interviewReview_df.dtypes

"""#### H1BCase"""

job_df

job_dict = {}
for index, row in job_df.iterrows():
  job_dict[(row['title'], row['fulltime'])] = row['jobId']

h1b_df

h1bCase_df = pd.DataFrame({ "emp_name": h1b_df['emp_name'], 
                            "job_title": h1b_df['job_title'],
                           'fulltime': h1b_df['full_time_position'],
                            "caseStatus": h1b_df['case_status'],
                            "caseYear": h1b_df['case_year'], 
                            "submitDate": h1b_df['case_submitted'],
                            "decisionDate": h1b_df['decision_date'],
                            "wageFrom": h1b_df['wage_from'],
                           })

h1bCase_df['companyId'] = h1bCase_df['emp_name'].apply(lambda x : company_dict[str(x)] if str(x) != 'nan' else np.nan)

h1bCase_df['companyId'] = pd.Series(h1bCase_df['companyId']).astype('Int64')

h1bCase_df['jobId'] = h1bCase_df.apply(lambda x : (job_dict[(x['job_title'], x['fulltime'])]), axis=1)

h1bCase_df.insert(loc=0, column='h1bCaseId', value= range(0, len(h1bCase_df)))

h1bCase_df.dtypes

"""#### HasRole"""

hasRole_df = pd.DataFrame({"company": h1b_df['emp_name'], 
                           "city": h1b_df['emp_city'],
                            "state": h1b_df['emp_state'],
                            "job_title": h1b_df['job_title'], 
                            "fulltime": h1b_df['full_time_position']})

location_df

location_dict = {}
for index, row in location_df.iterrows():
  location_dict[(row['city'], row['state'])] = row['locationId']

hasRole_df['locationId'] = hasRole_df.apply(lambda x : (location_dict[(x['city'], x['state'])]), axis=1)

hasRole_df['jobId'] = hasRole_df.apply(lambda x : (job_dict[(x['job_title'], x['fulltime'])]), axis=1)

hasRole_df['companyId'] = hasRole_df['company'].apply(lambda x : company_dict[x])

hasRole_df = hasRole_df.drop(columns=['company', 'city', 'state', 'job_title', 'fulltime'])

hasRole_df = hasRole_df.drop_duplicates(subset=['locationId', 'jobId', 'companyId']).reset_index(drop=True)

hasRole_df.insert(loc=0, column='hasRoleId', value= range(0, len(hasRole_df)))

hasRole_df.dtypes

"""#### InterestedIn (not populated until have users...?)

# Write Processed Dataset to Drive

**Uncomment and run ONLY IF you want to write to drive**
"""

# industry_df.to_csv('./drive/Shareddrives/550/datasets/updated_datasets/industry_df.csv')

# location_df.to_csv('./drive/Shareddrives/550/datasets/updated_datasets/location_df.csv')

# job_df.to_csv('./drive/Shareddrives/550/datasets/updated_datasets/job_df.csv')

# company_df.to_csv('./drive/Shareddrives/550/datasets/updated_datasets/company_df.csv')

# reviews_df.to_csv('./drive/Shareddrives/550/datasets/updated_datasets/reviews_df.csv')

# interviewReview_df.to_csv('./drive/Shareddrives/550/datasets/updated_datasets/interviewReview_df.csv')

# h1bCase_df.to_csv('./drive/Shareddrives/550/datasets/updated_datasets/h1bCase_df.csv')

# hasRole_df.to_csv('./drive/Shareddrives/550/datasets/updated_datasets/hasRole_df.csv')

"""#TODO: 
- Further string clean (to lower case, remove punctuation etc, and decide if it is necessary to calculate string similarity)

# Content below are outdated and needs to be modified

## Simple EDA
"""

def SimpleNumericAnalysis(df):
  numeric_df = df.select_dtypes(include=[np.number])
  print("Mean values analysis")
  print(numeric_df.apply(lambda col : col.mean()))
  print("\n")
  print("Standard deviation analysis")
  print(numeric_df.apply(lambda col : col.std()))
  print("\n")

def SimpleDbAnalysis(df):
  print("Dataset column types:\n", df.dtypes)
  print("\n")
  SimpleNumericAnalysis(df)
  print("Dataset size: ", len(df.index))